
\chapter{Introduction\label{ch:intro}}

Recently some phones are using face as an authentication way to make purchases, sign in to apps and unlock. In addition, there are many potential applications using face authentication as a convenient way of identifying a person such as surveillance and access control.

Many leading AI companies are trying their best to improve the accuracy and the robustness of the system. However, it is still not as accurate as compared to finger prints. This raise a question on how to balance of convenience and security. Sometimes users get annoyed by asking them to redo the identification again and again. Face recognition may not be 100\% correct, but as long as it is accurate within some acceptable range, it can be used to greatly improve the the usersâ€™ experience. According to global facial recognition market report, the market is projected to grow from USD 3.2 billion in 2019 to USD 7 billion by 2024 at the rate of 16.6\% .~\cite{globmarket}

The state-of-art face recognition models are mostly based on deep learning models. A number of face recognition applications have been proposed such as recognizing faces across pose and age ~\cite{cao2018vggface2} and facial-based intrusion detection system~\cite{amato2018facial}. Many AI companies are building their own facial recognition system for consumer use. Some of famous technology companies such as google and amazon are providing face recognition APIs on their own cloud platform. And many start-up companies such as Face++, Yitu, adn SenseTime are developing their own facial recognition systems(FRS) as well. All these FRS are built on top of deep learning models, which poses a concern on the stability and credibility of the systems.

Recent research has shown that the deep learning models are susceptible to intentionally generated inputs despite the fact that these deep learning models have achieved a great success in face recognition compared to traditional machine learning models. Those tampered inputs \textemdash \ known as adversarial examples~\cite{szegedy2013intriguing} \textemdash \ are relatively easy to generate for deep learning models. In particular, most of the adversarial examples are imperceptible to human beings yet able to fool the deep learning models.

Researchers have shown that deep learning models can be attacked by intentionally creating adversarial examples. A majority of the research conducted is to attack in the digital world. Goodfellow has shown an adversarial example on how a panda could be classified as gibbon with some noises added intentionally~\cite{goodfellow2014explaining}. A team at Berkeley managed to launch a successful attack on a commercial AI classification system~\cite{liu2016delving}. However, there are some research on attacks in the physical world as well. In 2017, another group demonstrated that the printing version on the adversarial generated image was able to to fool neural networks under different lighting and orientations~\cite{kurakin2016physical}. Detailed discussion on adversarial attacks will be shown in ~\ref{re-examples}.

In the field of FRS, there are attacks in the digital and physical world that as well such as \textit{Efficient Decision-based Black-box Adversarial Attacks on Face Recognition}~\cite{dong2019efficient} and \textit{Attacks on State-of-the-Art Face Recognition using Attentional Adversarial Attack Generative Network}~\cite{song2018attacks}. In 2016, Researchers in CMU proposed a methods of printing a pair of specially designed eyeglasses frames~\cite{sharif2016accessorize}.

In this dissertation, we study on the attacks in the physical world in the filed of FRS. More specifically, we call these attacks physically realizable because we are able to reproduce the generated faces in the real world. zWe use Multimodal Discriminant Analysis (MMDA)~\cite{sim2009simultaneous} method to generate the modified faces. Three different kinds of attacks: Break-in, Impersonation and Evasion are experimented on the public FRS Face++. In addition, further studies have been performed to discover how the sizes of the face database actually affect the attacks.